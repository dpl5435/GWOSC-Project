{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a2a21b4e",
      "metadata": {
        "editable": true,
        "id": "a2a21b4e",
        "tags": []
      },
      "source": [
        "<span style=\"float: left;padding: 1.3em\">![logo](https://github.com/gw-odw/odw/blob/main/Tutorials/logo.png?raw=1)</span>\n",
        "\n",
        "\n",
        "# Gravitational Wave Open Data Workshop\n",
        "\n",
        "## Tutorial 3.1: An introduction to parameter estimation\n",
        "\n",
        "In this tutorial, we will introduce Bayesian inference and \"parameter estimation\", a crucial part of gravitational wave astronomy. We will start with a general introduction to inference, the idea of stochastic sampling, and then introduce the\n",
        "[bilby Bayesian inference library](https://lscsoft.docs.ligo.org/bilby/).\n",
        "\n",
        "View this tutorial on [Google Colaboratory](https://colab.research.google.com/github/gw-odw/odw/blob/main/Tutorials/Day_3/Tuto_3.1_Introduction_to_parameter_estimation.ipynb) or launch [mybinder](https://mybinder.org/v2/gh/gw-odw/odw/HEAD)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba406484",
      "metadata": {
        "tags": [],
        "id": "ba406484"
      },
      "source": [
        "## Installation (execute only if running on a cloud platform, like Google Colab, or if you haven't done the installation already!)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c21ebf7",
      "metadata": {
        "editable": true,
        "id": "1c21ebf7",
        "tags": []
      },
      "source": [
        "> ⚠️ **Warning**: restart the runtime after running the cell below.\n",
        ">\n",
        "> To do so, click \"Runtime\" in the menu and choose \"Restart and run all\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f2014c53",
      "metadata": {
        "editable": true,
        "id": "f2014c53",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# -- Use the following line in Google Colab\n",
        "#! pip install -U -q bilby==2.4.0 matplotlib==3.10.0 dynesty==2.1.5 corner==2.2.3 scipy==1.12.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ece55ad",
      "metadata": {
        "id": "1ece55ad"
      },
      "source": [
        "### Download sample data\n",
        "\n",
        "If you are running on Google Colab, or another environment which copies the notebook but not the sample data. Please run the following cell. (Note: you may also download the file directly)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0714c2d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0714c2d9",
        "outputId": "974c205a-d970-4a4e-f7af-a1cb808da937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading toy_model.csv\n",
            "2025-05-28 21:09:33 URL:https://raw.githubusercontent.com/gw-odw/odw/main/Tutorials/Day_3/toy_model.csv [5104/5104] -> \"toy_model.csv\" [1]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if not os.path.isfile(\"toy_model.csv\"):\n",
        "  print(\"Downloading toy_model.csv\")\n",
        "  ! wget --no-verbose https://raw.githubusercontent.com/gw-odw/odw/main/Tutorials/Day_3/toy_model.csv\n",
        "else:\n",
        "  print(\"toy_model.csv exists; not downloading\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bfb97621",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "bfb97621",
        "outputId": "0b68ff4d-f66b-4a41-d33f-fce68137c279",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'bilby'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9a020bb99cf6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbilby\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bilby'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "# The first import of matplotlib can take some time (especially on cloud platforms). This is normal.\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import bilby\n",
        "import corner\n",
        "\n",
        "# Make bilby more terse\n",
        "bilby.core.utils.log.setup_logger(log_level='WARNING')\n",
        "\n",
        "for module in [np, matplotlib, bilby, corner]:\n",
        "    print(f\"Loaded {module.__name__}: {module.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a39352c6",
      "metadata": {
        "id": "a39352c6"
      },
      "source": [
        "### Bayesian inference\n",
        "\n",
        "Let us assume we are given some data $d$ and we wish to use the data to learn about a model $M$ with associated parameters $\\theta$. Bayes' theorem provides a framework to do this. Namely, the *posterior* probability distribution of the parameters $\\theta$ can be calculated from\n",
        "\n",
        "$$ p(\\theta | d, M) = \\frac{\\mathcal{L}(d| \\theta, M) \\;\\pi(\\theta | M)}{\\mathcal{Z}(d | M)} $$\n",
        "\n",
        "where $\\mathcal{L}(d| \\theta, M)$ is the *likelihood*, $\\pi(\\theta | M)$ is the *prior* probability distribution, and $\\mathcal{Z}(d | M)$ is the *evidence* (or fully marginalized likelihood) which normalized the posterior, i.e.\n",
        "\n",
        "$$ \\mathcal{Z}(d | M) = \\int \\mathcal{L}(d| \\theta, M) \\;\\pi(\\theta | M)\\, d\\theta $$\n",
        "\n",
        "For most interesting problems, the posterior and evidence (i.e. $p(\\theta | d, M) $ and $\\mathcal{Z}(d | M) $) cannot be calculated in closed form. Instead we rely on computational methods to approximate them - in this tutorials we will use **stochastic sampling**.\n",
        "\n",
        "Approximating $p(\\theta | d, M)$ is known as **parameter estimation** for obvious reasons. However, let us briefly note that Bayesian inference also extends to **model comparison** as follows. If we have two models, say $M_A$ and $M_B$, then Bayes theorem also tells us that\n",
        "\n",
        "$$ \\frac{P(M_A | d)}{P(M_B | d)} = \\frac{\\mathcal{Z}(d | M_A)}{\\mathcal{Z}(d | M_B)} \\frac{\\pi(M_A)}{\\pi(M_B)} $$\n",
        "\n",
        "This ratio known as the *posterior odds* compares the relative probability of each model. If it is greater than 1 this implies evidence for $M_A$ whereas if it is less than 1 it implies evidence for $M_B$. The right hand side is the product of the *Bayes factor* $\\frac{\\mathcal{Z}(d | M_A)}{\\mathcal{Z}(d | M_B)}$ (which can be estimated using stochastic sampling) and the *prior odds* $\\frac{\\pi(M_A)}{\\pi(M_B)}$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "101d6cdc",
      "metadata": {
        "id": "101d6cdc"
      },
      "source": [
        "### Introducing a toy model\n",
        "\n",
        "In order to introduce the basics of Bayesian inference and stochastic sampling, let us introduce some example data and then a toy model with parameters for us to estimate. In this repository, we include our data `toy_model.csv`. This is a text file containing an observed timeseries of `yobs` recorded at times `time`. We now read in the data and plot it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c38fa162",
      "metadata": {
        "id": "c38fa162",
        "tags": []
      },
      "outputs": [],
      "source": [
        "time, yobs = np.genfromtxt(\"toy_model.csv\", delimiter=\",\").T\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(time, yobs)\n",
        "ax.set(xlabel=\"Time [s]\", ylabel=\"Observed y values [arb. units]\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69afbe19",
      "metadata": {
        "id": "69afbe19"
      },
      "source": [
        "We are told by our theorist colleague, that a good model to explain this data is a so-called *sine Gaussian* which consists of a sinusoidal function modulated with a Gaussian-exponential envelope, i.e.\n",
        "\n",
        "$$ M_s: s(t) = e^{-(t/\\alpha)^2} \\sin\\left(2\\pi f t\\right)  $$\n",
        "\n",
        "where $M_s$ will be our label for the sine-Gaussian model. This model has two parameters $f$ and $\\alpha$; we could further generalise this to have an arbitrary amplitude, phase shift, or time shift, as well but here we introduce the simple two-parameter model for illustration.\n",
        "\n",
        "Before we go any further, let's check that we understand the model by plotting it out with some arbitrary choices for $f$ and $\\alpha$. To do this, we will create some functions that we can use later on. We plot the full function and the individual components to help visualise it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a63f0fd6",
      "metadata": {
        "id": "a63f0fd6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def sinusoid(time, freq):\n",
        "    return np.sin(2 * np.pi * freq * time)\n",
        "\n",
        "\n",
        "def gaussian_exponential(time, alpha):\n",
        "    return np.exp(-(time/alpha)**2)\n",
        "\n",
        "\n",
        "def model_Ms(time, freq, alpha):\n",
        "    return gaussian_exponential(time, alpha) * sinusoid(time, freq)\n",
        "\n",
        "freq = 2\n",
        "alpha = 0.5\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.plot(time, sinusoid(time, freq), label=\"Sinusoid\")\n",
        "ax.plot(time, gaussian_exponential(time, alpha), label=\"Gaussian exponential\")\n",
        "ax.plot(time, model_Ms(time, freq, alpha), label=\"Sine Gaussian\")\n",
        "ax.set(xlabel=\"Time [s]\", ylabel=\"Observed y values [arb. units]\")\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a47de4",
      "metadata": {
        "id": "b2a47de4"
      },
      "source": [
        "### Parameter Estimation\n",
        "\n",
        "Now that we have our data (`yobs`) and our model $M_s$, our task is to estimate the parameters $f$ and $\\alpha$. To do this, we will use Bayes theorem, i.e. we want to approximate the distribution\n",
        "\n",
        "$$ p(\\theta | d, M_s) = \\frac{\\mathcal{L}(d| \\theta, M_s) \\;\\pi(\\theta | M_s)}{\\mathcal{Z}(d | M_s)} $$\n",
        "\n",
        "where $\\theta=\\{f, \\alpha\\}$ is the two-dimensional parameter vector and $d$ is `yobs` (measured at times `time`). To this end, we need to define the likelihood and priors. Note that, if we are **only interested in the shape of the distribution**, then we can ignore the evidence, i.e. we can estimate the unnormalized distribution\n",
        "\n",
        "$$ p(\\theta | d, M_s) \\propto \\mathcal{L}(d| \\theta, M_s) \\;\\pi(\\theta | M_s) $$\n",
        "\n",
        "Before proceeding, we need to define our likelihood and prior.\n",
        "\n",
        "#### Parameter Estimation: Likelihood\n",
        "\n",
        "For this toy example, we will assume that the data consists of the generate model $M_s$ and additive white Gaussian noise, i.e.\n",
        "\n",
        "$$ y_{\\rm obs}(t) = s(t; f, \\alpha) + \\epsilon $$\n",
        "\n",
        "where $\\epsilon \\sim N(0, \\sigma)$ by which we mean that $\\epsilon$ is drawn from a Gaussian distribution with zero mean and standard deviation $\\sigma=0.1$ (for now, we will assume this is known a priori, but see challenges below for how it could be estimated).\n",
        "\n",
        "This definition of how the data was created allows us to define our likelihood. Namely, given a value of $\\{f, \\alpha\\}$, the likelihood of a single data point $y_i$ (measured at $t_i$) is:\n",
        "\n",
        "$$ \\mathcal{L}(y_i| f, \\alpha, M_s) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - s(t_i; f, \\alpha))^2}{2\\sigma^2}\\right) $$\n",
        "\n",
        "To extend this to multiple data points, we assume they are independent then\n",
        "\n",
        "$$ \\mathcal{L}(y_{obs} | f, \\alpha, M_s) = \\prod_i \\mathcal{L}(y_i| f, \\alpha, M_s) $$\n",
        "\n",
        "In practice, it is wise to work with the logarithm of the likelihood to avoid numerical overflow. Then, we have that\n",
        "\n",
        "$$ \\log \\mathcal{L}(y_{obs} | f, \\alpha, M_s) = \\sum_{i} -\\frac{1}{2}\\left(\\frac{\\left(y_i - s(t_i; f, \\alpha)\\right)^2}{\\sigma^2} + \\log\\left(2\\pi \\sigma^2\\right)\\right) $$\n",
        "\n",
        "We now transcribe this into `python`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27368b99",
      "metadata": {
        "id": "27368b99",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def log_likelihood_Ms(time, yobs, freq, alpha, sigma=0.1):\n",
        "    prediction = model_Ms(time, freq, alpha)\n",
        "    res = yobs - prediction\n",
        "    logl = -0.5 * (((res/sigma)**2) + np.log(2 * np.pi * sigma**2))\n",
        "    # Sum over all log_likelihoods (axis=0)\n",
        "    return np.sum(logl, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a9da691",
      "metadata": {
        "id": "3a9da691"
      },
      "source": [
        "#### Parameter Estimation: Priors\n",
        "\n",
        "The second part of Bayes theorem is the *prior*. For our two-component model, we will use a simple disjoint prior (i.e. $\\pi(\\theta | M_s)=\\pi(f| M_s)\\pi(\\alpha | M_s)$) with\n",
        "\n",
        "$$ \\pi(f| M_s) = \\textrm{Uniform}(0, 5) $$\n",
        "$$ \\pi(\\alpha| M_s) = \\textrm{Uniform}(0, 1) $$\n",
        "\n",
        "Let us create a python function to calculate the log of the prior:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef7322e",
      "metadata": {
        "id": "0ef7322e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def log_prior_Ms(freq, alpha):\n",
        "    \"\"\" Calculate the log prior under the Ms model\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    freq: array or float\n",
        "        The frequency at which to calculate the prior\n",
        "    alpha: array or float\n",
        "        The alpha at which to calculate the prior\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    log_prior: array\n",
        "        The log_prior calculated for all freq, alpha samples\n",
        "    \"\"\"\n",
        "    # Convert freq, alpha to numpy arrays\n",
        "    freq = np.atleast_1d(freq)\n",
        "    alpha = np.atleast_1d(alpha)\n",
        "\n",
        "    # Apply Uniform priors: calculate idxs of array where f, alpha in range\n",
        "    f_min = 0\n",
        "    f_max = 5\n",
        "    f_idxs = (freq > f_min) * (freq < f_max)\n",
        "\n",
        "    alpha_min = 0\n",
        "    alpha_max = 1\n",
        "    alpha_idxs = (alpha > alpha_min) * (alpha < alpha_max)\n",
        "\n",
        "    idxs = alpha_idxs * f_idxs\n",
        "\n",
        "    log_prior_volume = np.log(1/(f_max - f_min) * (1 / (alpha_max - alpha_min)))\n",
        "\n",
        "    log_prior = np.zeros_like(freq)\n",
        "    log_prior[idxs] = log_prior_volume\n",
        "    log_prior[~idxs] = -np.inf\n",
        "    return log_prior"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "290d7f24",
      "metadata": {
        "id": "290d7f24"
      },
      "source": [
        "#### Parameter Estimation: Rejection Sampling\n",
        "\n",
        "Now that we have our likelihood and prior, we will introduce **stochastic sampling**. We start by using the simplest type of stochastic sampling, rejection sampling. The idea is that to draw samples from a target distribution $p(\\theta | d, M_s)$ which is difficult to sample from, we first generate samples from a generating distribution $g(\\theta$) which is easy to sample from and then weight the samples relative to the target distribution. In practice you can choose any generating distribution you like, but we will use $g(\\theta) = g(f)g(\\alpha)$ where\n",
        "\n",
        "$$ g(f) = \\textrm{Uniform}(1.8, 2.2) $$\n",
        "$$ g(\\alpha) = \\textrm{Uniform}(0.2, 0.6) $$\n",
        "\n",
        "Our rejection sampling algorithm then proceeds as follows:\n",
        "\n",
        "\n",
        "1. Draw $\\theta'=[f, \\alpha]$ from $g(f)$ and $g(\\alpha)$\n",
        "2. Calculate the probability under the target and generating distributions (i.e. $p(\\theta' | d, M_s)$ and $g(\\theta')$)\n",
        "3. Calculate the weight $w=p(\\theta' | d, M_s) / g(\\theta')$\n",
        "4. Draw a random number $u$ uniformly distributed in $[0, 1]$\n",
        "5. If $w > u$, append $\\theta'$ to a set of samples, otherwise reject it and repeat\n",
        "\n",
        "Continue this loop until an acceptable number of samples have been drawn. The resulting set of samples are then an approximation to $p(\\theta | d, M_s)$ and be used to produce summary statistics or create plots.\n",
        "\n",
        "We now program the algorithm for our test data. However, there are two important differences between this algorithm and the expression above:\n",
        "\n",
        "1. We will work with the unnormalised distribution $p(\\theta | d, M_s)$ (i.e. we don't calculate the evidence $\\mathcal{Z}$. As a result, $w$ is also unnormalised and so it needs to be normalised before we apply step 5. Fortunately, we can normalize $w$ once we have a distribution of values.  \n",
        "2. For computational efficiency, rather than using a while loop we will instead draw a set of 100,000 samples, calculate the weights for each, and then apply rejection sampling. This utilises numpy array optimization and also enables us to normalise the weights to a distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4f02fb9",
      "metadata": {
        "id": "d4f02fb9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Draw 100,000 samples from g(theta)\n",
        "N = 100000\n",
        "freq_gsamples = np.random.uniform(1.8, 2.2, N)\n",
        "alpha_gsamples = np.random.uniform(0.2, 0.6, N)\n",
        "\n",
        "# Make time a 2D array to enable broadcasting across the samples\n",
        "time_array = time[:, np.newaxis]\n",
        "yobs_array = yobs[:, np.newaxis]\n",
        "\n",
        "# Calculate the log_likelihood and log_prior for all samples\n",
        "log_likelihood_vals = log_likelihood_Ms(time_array, yobs_array, freq_gsamples, alpha_gsamples)\n",
        "log_prior_vals = log_prior_Ms(freq_gsamples, alpha_gsamples)\n",
        "log_posterior_vals = log_likelihood_vals + log_prior_vals\n",
        "\n",
        "# Calculate the weights\n",
        "weights = np.exp(log_posterior_vals)\n",
        "\n",
        "# Normalise the weights\n",
        "weights = weights / max(weights)\n",
        "\n",
        "# Rejection sample\n",
        "keep = weights > np.random.uniform(0, 1, weights.shape)\n",
        "alpha_samples = alpha_gsamples[keep]\n",
        "freq_samples = freq_gsamples[keep]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55f19fa4",
      "metadata": {
        "id": "55f19fa4"
      },
      "source": [
        "The end result of this is a set of samples `freq_samples` and `alpha_samples` that approximate the posterior distribution. We can get a quick visualisation of these by using the `corner` package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "013d1029",
      "metadata": {
        "id": "013d1029",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Create a corner plot\n",
        "samples = np.array([freq_samples, alpha_samples]).T\n",
        "fig = corner.corner(samples, bins=20, labels=[\"f\", \"alpha\"], show_titles=True, quantiles=[0.16, 0.5, 0.84],)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9671d0f9",
      "metadata": {
        "id": "9671d0f9"
      },
      "source": [
        "The plot above shows two 1D histograms (one for each parameter) and one 2D histogram (showing any correlations between the samples). Areas where the posterior is large (i.e. the histogram count is high) represent the most probable values of $f$ and $\\alpha$ which explain the data.\n",
        "\n",
        "The samples can also be used to provide a summary statistic. For example, if you wanted to report the mean and standard deviation interval for $f$, you could do something like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b129a93",
      "metadata": {
        "id": "6b129a93",
        "tags": []
      },
      "outputs": [],
      "source": [
        "mean_f = np.mean(freq_samples)\n",
        "std_f = np.std(freq_samples)\n",
        "print(f\"We estimate the mean and standard deviation of frequency to be {mean_f:0.2f}+/-{std_f:0.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad2e9e07",
      "metadata": {
        "id": "ad2e9e07"
      },
      "source": [
        "Typically, in GW astronomy, we use the median and a 90\\% credible interval because the posterior is often non-symmetric."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15f4025c",
      "metadata": {
        "id": "15f4025c"
      },
      "source": [
        "In this exercise, we have learned that rejection sampling can be used to approximate the posterior distribution. However, we should note that it is highly inefficienct. It works okay here, because we tightly tuned the edges of $g(\\theta)$, but if you go back and increase these to a wider range, you'll see the efficiency quickly drops off. Moreover, the efficiency of rejection sampling also suffers when we start to look at problems in more than 2 parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b50806fe",
      "metadata": {
        "id": "b50806fe",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# To calculate the efficiency\n",
        "efficiency = len(freq_samples) / len(freq_gsamples)\n",
        "print(efficiency)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "983f5090",
      "metadata": {
        "id": "983f5090"
      },
      "source": [
        "#### Parameter Estimation: Markov Chain Monte Carlo\n",
        "\n",
        "The rejection sampling algorithm is inefficient in problems where the posterior is small compared to the prior volume. To address this, a range of stochastic sampling algorithms are available. In GW astronomy, two are preferred: Markov Chain Monte Carlo (MCMC), and Nested Sampling. We will now introduce `bilby` which is an inference package allowing access to several off the shelf samplers.\n",
        "\n",
        "`bilby` has helper functionality to program up our likelihood and prior. We begin by using that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d959266a",
      "metadata": {
        "id": "d959266a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "likelihood_Ms = bilby.core.likelihood.GaussianLikelihood(time, yobs, model_Ms, sigma=0.1)\n",
        "\n",
        "priors_Ms = dict(\n",
        "    freq=bilby.core.prior.Uniform(0, 5),\n",
        "    alpha=bilby.core.prior.Uniform(0, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db612cb2",
      "metadata": {
        "id": "db612cb2"
      },
      "source": [
        "This creates two objects, a `likelihood` and `prior`. These can now be passed to `run_sampler` with some options to select the `bilby_mcmc` sampler and configure it in a way that is efficient for this problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0de4f9f5",
      "metadata": {
        "id": "0de4f9f5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "result_Ms = bilby.run_sampler(\n",
        "    likelihood_Ms,\n",
        "    priors_Ms,\n",
        "    label=\"model_Ms\",  # A label to help sort our results\n",
        "    outdir=\"toy_model\",  # The directory where results will be kept\n",
        "    clean=True,  # Don't use cached data\n",
        "    sampler=\"bilby_mcmc\",  # Use the bilby_mcmc sampler\n",
        "    nsamples=1000,  # Draw 1000 samples\n",
        "    printdt=2,  # Print an update ever 5 seconds\n",
        "    L1steps=1,  # The number of internal steps to take\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aee65da2",
      "metadata": {
        "id": "aee65da2"
      },
      "source": [
        "The `run_sampler` method returns an object `result`. This contains all the interesting information that we might want to use. It also has methods we can use. For example, let's create a corner plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f25ad87",
      "metadata": {
        "id": "9f25ad87",
        "tags": []
      },
      "outputs": [],
      "source": [
        "result_Ms.plot_corner()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c789bfba",
      "metadata": {
        "id": "c789bfba"
      },
      "source": [
        "We can also get to the samples, which are stored as a `pandas` data frame, as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "423bd1c4",
      "metadata": {
        "id": "423bd1c4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "result_Ms.posterior"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dacade6",
      "metadata": {
        "editable": true,
        "id": "6dacade6",
        "tags": []
      },
      "source": [
        "Let's compare our rejection sampling results with the `bilby_mcmc` results. To do this, we will create a histogram of the $f$ samples for both:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "104209a8",
      "metadata": {
        "id": "104209a8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.hist(freq_samples, density=True, bins=50, label=\"Rejection Sampling\", alpha=0.8)\n",
        "ax.hist(result_Ms.posterior[\"freq\"], density=True, bins=50, label=\"bilby_mcmc\", alpha=0.5)\n",
        "ax.set(xlabel=\"f\", ylabel=\"Density\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b4d86aa",
      "metadata": {
        "id": "7b4d86aa"
      },
      "source": [
        "They lie on top of each other: this indicates that, within sampling errors, they are producing identical results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16908bdb",
      "metadata": {
        "id": "16908bdb"
      },
      "source": [
        "## Challenge Questions\n",
        "\n",
        "### Q1.1\n",
        "To quantify agreement between distributions,  it is often useful to calculate a summary statistic and show they agree. Calculate the mean and standard deviation from the rejection sampling algorithm **and** the `bilby_mcmc` approach for both $f$ and $\\alpha$. Do they agree to within the quoted standard deviation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bee4e811",
      "metadata": {
        "id": "bee4e811"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "999abd53",
      "metadata": {
        "id": "999abd53"
      },
      "source": [
        "### Q1.2\n",
        "Having the posterior distributions tells us about the parameters, but not if the model is a good fit to the data. For this, it is useful to use a *posterior predictive check*. Create a plot of the raw data (i.e `yobs` against `time`) Then, overlay on it the prediction of your model using the mean for $f$ and $\\alpha$ estimated in Q1 (from either method)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bf35cde",
      "metadata": {
        "id": "6bf35cde"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c407941d",
      "metadata": {
        "id": "c407941d"
      },
      "source": [
        "### Q2.1 (Optional)\n",
        "It is more common to show the maximum likelihood fit and some measure of uncertainty. For the maximum likelihood, this can be obtained from the `posterior` data frame as follow:\n",
        "\n",
        "- First identify the index of the maximum likelihood point `idx_max = np.argmax(result_Ms.posterior.log_likelihood)`\n",
        "- Then return the row of the maximum `maxL = result_Ms.posterior.iloc[idx_max]`\n",
        "- The values can be accessed with `maxL.freq` for example\n",
        "\n",
        "A straightforward approach is to draw random samples from $p(\\theta| d, M_s)$ and plot these. You can draw a random sample from the bilby posterior using `result.posterior.sample().iloc[0]` (this will give you a single sample with keys for `freq` and `alpha`). Draw 100 samples from the posterior and overplot them alongside your mean estimate. You will want to add `color='C3'`, `zorder=-100` and `alpha=0.1` to the `plot()` command so they don't overwhelm the rest of the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7e18d60",
      "metadata": {
        "id": "f7e18d60"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c24e267e",
      "metadata": {
        "id": "c24e267e"
      },
      "source": [
        "### Q3.1 (Optional: hard)\n",
        "Your plot from Q2 should tell you that the model isn't fully explaining the data! You take the plot back to your theorist colleague and demand an explanation. They quickly tell you that actually there is a better model where the frequency of the sinusoid is time-dependent:\n",
        "\n",
        "$$ M_t: s(t) = e^{-(t/\\alpha)^2} \\sin\\left(2\\pi (f_0 + \\dot{f} \\times t) t\\right)  $$\n",
        "\n",
        "This new model has three parameters: $f$, $\\dot{f}$ and $\\alpha$.\n",
        "\n",
        "Your task is to program this model in a new function, then run `bilby_mcmc` on it and reproduce the figure from Q3. You will need to define new parameters $f_0$ and $\\dot{f}$ in the function (probably best to call it `model_Mt` to be consistent) and add these to the prior. If you are successful, the predicted plot should nicely overlap the data. Along the way you should also create a corner plot: this can be helpful to see if you have chosen appropriate priors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e81bdc0",
      "metadata": {
        "id": "1e81bdc0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b27a0dab",
      "metadata": {
        "id": "b27a0dab"
      },
      "source": [
        "### Q3.2 (Optional)\n",
        "We can tell already that the $M_t$ model is far better than $M_s$: the posterior predictive fit better explains the data. One can also look at the posterior of $\\dot{f}$ from $M_t$: Since $M_s$ is a subset of $M_t$ (i.e. they are equivalent when $\\dot{f}=0$) the fact that the posterior on $\\dot{f}$ peaks away from zero tells you that $M_t$ is better than $M_s$.\n",
        "\n",
        "However, it is fun to also show how we can quantify this using the posterior odds (defined in the first section!). For this, we need to calculate the evidence $\\mathcal{Z}(d | M_s)$ and $\\mathcal{Z}(d | M_t)$. They can be estimated by stochastic sampling. However, `bilby_mcmc` is not the right tool (in can in principle, but the alternative is superior for evidence calculation). Instead, we should use the **nested sampling** package `dynesty`\n",
        "\n",
        "- Rerun your analysis of both models, but use `sampler=\"dynesty\"` in `run_sampler`. You will also need to add configuration arguments: `nlive=500, sample=\"rwalk\"`)\n",
        "- Calculate the Bayes factor using `np.exp(result_Mt.log_evidence - result_Ms.log_evidence)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37bc8438",
      "metadata": {
        "id": "37bc8438"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d037aab1",
      "metadata": {
        "id": "d037aab1"
      },
      "source": [
        "### Q4.1 (Optional: easy)\n",
        "\n",
        "Throughout all our analyses, we assumed that $\\sigma=0.1$. But, what would we do if we didn't know this a priori? One option is to estimate it, you can do this by simply taking the standard deviation of data away from where the transient signal is. First, try to cut out the first 1s of data and use this to estimate $\\sigma$, is it close to the true value $0.1$? How about taking the last second of data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1a02fd6",
      "metadata": {
        "id": "e1a02fd6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a48e5fa7",
      "metadata": {
        "id": "a48e5fa7"
      },
      "source": [
        "### Q4.2 (Optional: harder)\n",
        "\n",
        "If we did not know $\\sigma$ a priori, and we didn't trust our estimate, we could simply infer it from the data! You can do this by removing the `sigma=0.1` in the `GaussianLikelihood` and setting a prior on sigma. Try this using your $M_t$ model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f8fc050",
      "metadata": {
        "editable": true,
        "id": "2f8fc050",
        "tags": []
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}