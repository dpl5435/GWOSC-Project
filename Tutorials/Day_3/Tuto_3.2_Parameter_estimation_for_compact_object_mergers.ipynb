{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "editable": true,
        "id": "OjUNeFsxyguu",
        "tags": []
      },
      "source": [
        "<span style=\"float: left;padding: 1.3em\">![logo](https://github.com/gw-odw/odw/blob/main/Tutorials/logo.png?raw=1)</span>\n",
        "\n",
        "\n",
        "# Gravitational Wave Open Data Workshop\n",
        "\n",
        "## Tutorial 3.2: Parameter estimation for compact object mergers\n",
        "\n",
        "In this tutorial, we will learn how to run our own parameter estimation analysis for compact object mergers using the [bilby Bayesian inference library](https://lscsoft.docs.ligo.org/bilby/). Specifically, we will analyse the first detection, GW150914 using open data. Typically, a full analysis using stochastic sampling on a personal computer can take many hours if not days. Therefore, this analysis will be restricted to a non-spinning binary black hole model and neglect the marginalization of the calibration uncertainty. This will take about 30 minutes to run (note that a bottleneck is given by the first cell in the Section \"Create a likelihood-Run the analysis\": computational time depends a lot on the machine you decide to run, on Google Colab is about 10 minutes this cell only).\n",
        "   \n",
        "You can find more examples of using bilby here: https://lscsoft.docs.ligo.org/bilby/examples.html\n",
        "\n",
        "View this tutorial on [Google Colaboratory](https://colab.research.google.com/github/gw-odw/odw/blob/main/Tutorials/Day_3/Tuto_3.2_Parameter_estimation_for_compact_object_mergers.ipynb) or launch [mybinder](https://mybinder.org/v2/gh/gw-odw/odw/HEAD).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ21KPmiwBrx"
      },
      "outputs": [],
      "source": [
        "# Those 2 lines are just to avoid some harmless warnings when importing packages\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", \"Wswiglal-redir-stdio\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "editable": true,
        "id": "VwsIdKJ3yguv",
        "tags": []
      },
      "source": [
        "## Installation (execute only if running on a cloud platform, like Google Colab, or if you haven't done the installation already!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "editable": true,
        "id": "f0q3Y_9gygu0",
        "tags": []
      },
      "source": [
        "> ⚠️ **Warning**: restart the runtime after running the cell below.\n",
        ">\n",
        "> To do so, click \"Runtime\" in the menu and choose \"Restart and run all\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "editable": true,
        "id": "eJeo4XrHyguw",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# -- Use the following line in Google Colab\n",
        "! pip install -U -q bilby==2.4.0 matplotlib==3.10.0 dynesty==2.1.5 corner==2.2.3 gwpy==3.0.12 lalsuite==7.25 scipy==1.12.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "XK8fHu13ygu1"
      },
      "source": [
        "## Initialization\n",
        "\n",
        "We begin by importing some commonly used functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "id": "HyRSGt6cygu2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# The first import of matplotlib can take some time (especially on cloud platforms). This is normal.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import bilby\n",
        "from bilby.core.prior import Uniform, PowerLaw\n",
        "from bilby.gw.conversion import convert_to_lal_binary_black_hole_parameters, generate_all_bbh_parameters\n",
        "\n",
        "# Make bilby more terse\n",
        "bilby.core.utils.log.setup_logger(log_level='WARNING')\n",
        "\n",
        "from gwpy.timeseries import TimeSeries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1x32gW4wBry"
      },
      "source": [
        "## Bilby version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "QaOuZNLBwBry"
      },
      "outputs": [],
      "source": [
        "print(bilby.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "_Hd4d4KVygu6"
      },
      "source": [
        "## Getting the data: GW150914\n",
        "\n",
        "In this notebook, we'll analyse GW150914. Our first task is to obtain some data!\n",
        "\n",
        "We need to know the trigger time. This can be found on the [GWOSC page](https://gwosc.org/events/GW150914/), here we define it as a variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "id": "1cUhLaFIygu6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "time_of_event = 1126259462.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xezhtEDwBry"
      },
      "source": [
        "### Set up empty interferometers\n",
        "\n",
        "We need to get some data to analyse. We'll be using data from the Hanford (H1) and Livingston (L1) ground-based gravitational wave detectors. To organise ourselves, we'll create two \"empty\" interferometers. These are empty in the sense that they don't have any strain data. But, they know about the orientation and location of their respective namesakes. It may also be interesting to note that they are initialised with the planned design sensitivity power spectral density of advanced LIGO - we'll overwrite this later on, but it is often useful for simulations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "p8fFu6b2wBry"
      },
      "outputs": [],
      "source": [
        "H1 = bilby.gw.detector.get_empty_interferometer(\"H1\")\n",
        "L1 = bilby.gw.detector.get_empty_interferometer(\"L1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1pjsEwSwBrz"
      },
      "source": [
        "### Download the data\n",
        "\n",
        "To analyse the signal, we need to download analysis data. Here, we will use [gwpy](https://gwpy.github.io/) to download the open strain data. For a general introduction to reading/writing data with gwpy, see [the documentation](https://gwpy.github.io/docs/stable/timeseries/).\n",
        "\n",
        "To analyse GW150914, we will use a 4s period duration centered on the event itself. It is standard to choose the data such that it always includes a \"post trigger duration\" of 2s. That is, there is always 2s of data after the trigger time. We therefore define all times relative to the trigger time, duration and this post-trigger duration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Y3lNPqkYwBrz"
      },
      "outputs": [],
      "source": [
        "# Definite times in relation to the trigger time (time_of_event), duration and post_trigger_duration\n",
        "post_trigger_duration = 2\n",
        "duration = 4\n",
        "analysis_start = time_of_event + post_trigger_duration - duration\n",
        "\n",
        "# Use gwpy to fetch the open data\n",
        "H1_analysis_data = TimeSeries.fetch_open_data(\n",
        "    \"H1\", analysis_start, analysis_start + duration, sample_rate=4096, cache=True)\n",
        "\n",
        "L1_analysis_data = TimeSeries.fetch_open_data(\n",
        "    \"L1\", analysis_start, analysis_start + duration, sample_rate=4096, cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyfg1tyNwBrz"
      },
      "source": [
        "Here, `H1_analysis_data` and its L1 counterpart are gwpy `TimeSeries` objects. As such, we can plot the out data itself:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "mHb40D9xwBrz"
      },
      "outputs": [],
      "source": [
        "H1_analysis_data.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMg1PVtiwBrz"
      },
      "source": [
        "This doesn't tell us much of course! It is dominated by the low frequency noise.\n",
        "\n",
        "### Initialise the Bilby interferometers with the strain data\n",
        "\n",
        "Now, we pass the downloaded strain data to our `H1` and `L1` Bilby interferometer objects. For other methods to set the strain data, see the various `set_strain_data*` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "RpXnSgpAwBrz"
      },
      "outputs": [],
      "source": [
        "H1.set_strain_data_from_gwpy_timeseries(H1_analysis_data)\n",
        "L1.set_strain_data_from_gwpy_timeseries(L1_analysis_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbQLjvVuwBrz"
      },
      "source": [
        "### Download the power spectral data\n",
        "\n",
        "Parameter estimation relies on having a power spectral density (PSD) - an estimate of the coloured noise properties of the data. Here, we will create a PSD using off-source data. For a review of methods to estimate PSDs, see, e.g. [Chatziioannou et al. (2019)](https://ui.adsabs.harvard.edu/abs/2019PhRvD.100j4004C/abstract).\n",
        "\n",
        "Again, we need to download this from the open strain data. We start by figuring out the amount of data needed - in this case 32 times the analysis duration. We fetch the segment with this duration immediately preceding the analysis segment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "fmF4A2DFwBrz"
      },
      "outputs": [],
      "source": [
        "psd_duration = duration * 32\n",
        "psd_start_time = analysis_start - psd_duration\n",
        "\n",
        "H1_psd_data = TimeSeries.fetch_open_data(\n",
        "    \"H1\", psd_start_time, psd_start_time + psd_duration, sample_rate=4096, cache=True)\n",
        "\n",
        "L1_psd_data = TimeSeries.fetch_open_data(\n",
        "    \"L1\", psd_start_time, psd_start_time + psd_duration, sample_rate=4096, cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJY2ViLZwBrz"
      },
      "source": [
        "Having obtained the data to generate the PSD, we now use the standard [gwpy psd](https://gwpy.github.io/docs/stable/api/gwpy.timeseries.TimeSeries/#gwpy.timeseries.TimeSeries.psd) method to calculate the PSD. Here, the `psd_alpha` variable is converting the `roll_off` applied to the strain data into the fractional value used by `gwpy`. This applies a window with an appropriate shape to the time-domain data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "9OH3iQk8wBrz"
      },
      "outputs": [],
      "source": [
        "psd_alpha = 2 * H1.strain_data.roll_off / duration\n",
        "H1_psd = H1_psd_data.psd(fftlength=duration, overlap=0, window=(\"tukey\", psd_alpha), method=\"median\")\n",
        "L1_psd = L1_psd_data.psd(fftlength=duration, overlap=0, window=(\"tukey\", psd_alpha), method=\"median\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzva533DwBrz"
      },
      "source": [
        "### Initialise the PSD\n",
        "Now that we have PSDs for H1 and L1, we can overwrite the `power_spectal_density` attribute of our interferometers with a new PSD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "lskYKbvrwBrz"
      },
      "outputs": [],
      "source": [
        "H1.power_spectral_density = bilby.gw.detector.PowerSpectralDensity(\n",
        "    frequency_array=H1_psd.frequencies.value, psd_array=H1_psd.value)\n",
        "L1.power_spectral_density = bilby.gw.detector.PowerSpectralDensity(\n",
        "    frequency_array=H1_psd.frequencies.value, psd_array=L1_psd.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSdAvpw-wBrz"
      },
      "source": [
        "### Looking at the data\n",
        "Okay, we have spent a bit of time now downloading and initializing things. Let's check that everything makes sense. To do this, we'll plot our analysis data alongside the amplitude spectral density (ASD); this is just the square root of the PSD and has the right units to be comparable to the frequency-domain strain data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "NoJx8TiKwBrz"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "idxs = H1.strain_data.frequency_mask  # This is a boolean mask of the frequencies which we'll use in the analysis\n",
        "ax.loglog(H1.strain_data.frequency_array[idxs],\n",
        "          np.abs(H1.strain_data.frequency_domain_strain[idxs]))\n",
        "ax.loglog(H1.power_spectral_density.frequency_array[idxs],\n",
        "          H1.power_spectral_density.asd_array[idxs])\n",
        "ax.set_xlabel(\"Frequency [Hz]\")\n",
        "ax.set_ylabel(\"Strain [strain/$\\sqrt{Hz}$]\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pn5dXuowBrz"
      },
      "source": [
        "What is happening at high frequencies? This is an artifact of the downsampling applied to the data - note that we downloaded the 4096Hz data which is downsampled from 16384Hz. We aren't really interested in the data at these high frequencies so let's adjust the maximum frequency used in the analysis to 1024 Hz and plot things again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ob-842NuwBrz"
      },
      "outputs": [],
      "source": [
        "H1.maximum_frequency = 1024\n",
        "L1.maximum_frequency = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "JzCALXfrwBrz"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "idxs = H1.strain_data.frequency_mask\n",
        "ax.loglog(H1.strain_data.frequency_array[idxs],\n",
        "          np.abs(H1.strain_data.frequency_domain_strain[idxs]))\n",
        "ax.loglog(H1.power_spectral_density.frequency_array[idxs],\n",
        "          H1.power_spectral_density.asd_array[idxs])\n",
        "ax.set_xlabel(\"Frequency [Hz]\")\n",
        "ax.set_ylabel(\"Strain [strain/$\\sqrt{Hz}$]\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JJDt0ebwBrz"
      },
      "source": [
        "Okay, that is better - we now won't analyse any data near to the artifact produced by downsampling. Now we have some sensible data to analyse so let's get right on with doing the analysis!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gANAndgwBrz"
      },
      "source": [
        "## Low dimensional analysis\n",
        "\n",
        "In general a compact binary coalescence signal is described by 15 parameters describing the masses, spins, orientation, and position of the two compact objects along with a time at which the signal merges. The goal of parameter estimation is to figure out what the data (and any cogent prior information) can tell us about the likely values of these parameters - this is called the \"posterior distribution of the parameters\".\n",
        "\n",
        "To start with, we'll analyse the data fixing all but a few of the parameters to known values (in Bayesian lingo - we use delta function priors), this will enable us to run things in a few minutes rather than the many hours needed to do full parameter estimation.\n",
        "\n",
        "We'll start by thinking about the mass of the system. We call the heavier black hole the primary and label its mass $m_1$ and that of the secondary (lighter) black hole $m_2$. In this way, we always define $m_1 \\ge m_2$. It turns out that inferences about $m_1$ and $m_2$ are highly correlated, we'll see exactly what this means later on.\n",
        "\n",
        "Bayesian inference methods are powerful at figuring out highly correlated posteriors. But, we can help it along by sampling in parameters which are not highly correlated. In particular, we define a new parameter called the [chirp mass](https://en.wikipedia.org/wiki/Chirp_mass) to be\n",
        "\n",
        "$$ \\mathcal{M} = \\frac{(m_1 m_2)^{3/5}}{(m_1 + m_2)^{1/5}} $$\n",
        "\n",
        "and the mass ratio\n",
        "\n",
        "$$ q = \\frac{m_{2}}{m_1} $$\n",
        "\n",
        "If we sample (make inferences about) $\\mathcal{M}$ and $q$, our code is much faster than if we use $m_1$ and $m_2$ directly! Note that so long as equivalent prior is given - one can also sample in the component masses themselves and you will get the same answer, it is just much slower!\n",
        "\n",
        "Once we have inferred $\\mathcal{M}$ and $q$, we can then derive $m_1$ and $m_2$ from the resulting samples (we'll do that in just a moment).\n",
        "\n",
        "Okay, let's run a short (~1min on a single 2.8GHz core), low-dimensional parameter estimation analysis. This is done by defining a prior dictionary where all parameters are fixed, except those that we want to vary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUcbOOD6wBrz"
      },
      "source": [
        "### Create a prior\n",
        "\n",
        "Here, we create a prior fixing everything except the chirp mass, mass ratio, phase and geocent_time parameters to fixed values. The first two were described above. The second two give the phase of the system and the time at which it merges referred to the centre of the Earth (which is the standard reference considering that different detectors detect the same signal at appreciable different times, depending on the source location in the sky)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "XjwdOXDxwBrz"
      },
      "outputs": [],
      "source": [
        "prior = bilby.core.prior.PriorDict()\n",
        "prior['chirp_mass'] = Uniform(name='chirp_mass', minimum=30.0,maximum=32.5)\n",
        "prior['mass_ratio'] = Uniform(name='mass_ratio', minimum=0.5, maximum=1)\n",
        "prior['phase'] = Uniform(name=\"phase\", minimum=0, maximum=2*np.pi)\n",
        "prior['geocent_time'] = Uniform(name=\"geocent_time\", minimum=time_of_event-0.1, maximum=time_of_event+0.1)\n",
        "prior['a_1'] =  0.0\n",
        "prior['a_2'] =  0.0\n",
        "prior['tilt_1'] =  0.0\n",
        "prior['tilt_2'] =  0.0\n",
        "prior['phi_12'] =  0.0\n",
        "prior['phi_jl'] =  0.0\n",
        "prior['dec'] =  -1.2232\n",
        "prior['ra'] =  2.19432\n",
        "prior['theta_jn'] =  1.89694\n",
        "prior['psi'] =  0.532268\n",
        "prior['luminosity_distance'] = PowerLaw(alpha=2, name='luminosity_distance', minimum=50, maximum=2000, unit='Mpc', latex_label='$d_L$')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZRrXs0dwBrz"
      },
      "source": [
        "## Create a likelihood\n",
        "\n",
        "For Bayesian inference, we need to evaluate the likelihood. In Bilby, we create a likelihood object. This is the communication interface between the sampling part of Bilby and the data. Explicitly, when Bilby is sampling it only uses the `parameters` and `log_likelihood()` of the likelihood object. This means the likelihood can be arbitrarily complicated and the sampling part of Bilby won't mind a bit!\n",
        "\n",
        "Let's create a `GravitationalWaveTransient`, a special inbuilt method carefully designed to wrap up evaluating the likelihood of a waveform model in some data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "_NKAWeqpwBrz"
      },
      "outputs": [],
      "source": [
        "# First, put our \"data\" created above into a list of interferometers (the order is arbitrary)\n",
        "interferometers = [H1, L1]\n",
        "\n",
        "# Next create a dictionary of arguments which we pass into the LALSimulation waveform - we specify the waveform approximant here\n",
        "waveform_arguments = dict(\n",
        "    waveform_approximant='IMRPhenomXP', reference_frequency=100., catch_waveform_errors=True)\n",
        "\n",
        "# Next, create a waveform_generator object. This wraps up some of the jobs of converting between parameters etc\n",
        "waveform_generator = bilby.gw.WaveformGenerator(\n",
        "    frequency_domain_source_model=bilby.gw.source.lal_binary_black_hole,\n",
        "    waveform_arguments=waveform_arguments,\n",
        "    parameter_conversion=convert_to_lal_binary_black_hole_parameters)\n",
        "\n",
        "# Finally, create our likelihood, passing in what is needed to get going\n",
        "likelihood = bilby.gw.likelihood.GravitationalWaveTransient(\n",
        "    interferometers, waveform_generator, priors=prior,\n",
        "    time_marginalization=True, phase_marginalization=True, distance_marginalization=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeI1k_RVwBr0"
      },
      "source": [
        "Note that we also specify `time_marginalization=True`, `phase_marginalization=True`, and `distance_marginalization=True`. This is a trick often used in Bayesian inference. We analytically marginalize (integrate) over the time/phase of the system while sampling, effectively reducing the parameter space and making it easier to sample. Bilby will then figure out (after the sampling) posteriors for these marginalized parameters. For an introduction to this topic, see [Thrane & Talbot (2019)](https://arxiv.org/abs/1809.02293)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lI5C_lxCwBr0"
      },
      "source": [
        "### Run the analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "LCfygeVyygvM"
      },
      "source": [
        "Now that the prior is set-up and the likelihood is set-up (with the data and the signal mode), we can run the sampler to get the posterior result. This function takes the likelihood and prior along with some options for how to do the sampling and how to save the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "id": "HHS9JSX3ygvN",
        "tags": []
      },
      "outputs": [],
      "source": [
        "result_short = bilby.run_sampler(\n",
        "    likelihood, prior, sampler='dynesty', outdir='short', label=\"GW150914\",\n",
        "    conversion_function=bilby.gw.conversion.generate_all_bbh_parameters,\n",
        "    nlive=250, dlogz=1.,  # <- Arguments are used to make things fast - not recommended for general use\n",
        "    clean=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I95hGjlzwBr3"
      },
      "source": [
        "### Looking at the outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "wKR045TIygvT"
      },
      "source": [
        "The `run_sampler` returned `result_short` - this is a Bilby result object. The posterior samples are stored in a [pandas data frame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) (think of this like a spreadsheet); let's take a look at it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "jOdQEPbzwBr3"
      },
      "outputs": [],
      "source": [
        "result_short.posterior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeJ9GP1gwBr3"
      },
      "source": [
        "We can pull out specific parameters that we are interested in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "0SeIHvNnwBr3"
      },
      "outputs": [],
      "source": [
        "result_short.posterior[\"chirp_mass\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6gR-LDfwBr3"
      },
      "source": [
        "This returned another `pandas` object. If you just want to get the numbers as a numpy array run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Z5NyR6_XwBr3"
      },
      "outputs": [],
      "source": [
        "Mc = result_short.posterior[\"chirp_mass\"].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koLZaC9UwBr3"
      },
      "source": [
        "We can then get some useful quantities such as the 90\\% credible interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "LAcM3IFLwBr3"
      },
      "outputs": [],
      "source": [
        "lower_bound = np.quantile(Mc, 0.05)\n",
        "upper_bound = np.quantile(Mc, 0.95)\n",
        "median = np.quantile(Mc, 0.5)\n",
        "print(\"Mc = {} with a 90% C.I = {} -> {}\".format(median, lower_bound, upper_bound))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMtDff2UwBr3"
      },
      "source": [
        "We can then plot the chirp mass in a histogram adding a region to indicate the 90\\% C.I."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "44zn9-xOwBr3"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.hist(result_short.posterior[\"chirp_mass\"], bins=20)\n",
        "ax.axvspan(lower_bound, upper_bound, color='C1', alpha=0.4)\n",
        "ax.axvline(median, color='C1')\n",
        "ax.set_xlabel(\"chirp mass\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8BD6YaYwBr3"
      },
      "source": [
        "The result object also has in-built methods to make nice plots such as corner plots. You can add the priors if you are only plotting parameters which you sampled in, e.g."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "b_QRDqkzwBr3"
      },
      "outputs": [],
      "source": [
        "result_short.plot_corner(parameters=[\"chirp_mass\", \"mass_ratio\", \"geocent_time\", \"phase\"], prior=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8EkzVRpwBr3"
      },
      "source": [
        "You can also plot lines indicating specific points. Here, we add the values recorded on [GWOSC](https://gwosc.org/events/GW150914/). Notably, these fall outside the bulk of the posterior uncertainty here. This is because we limited our prior to a non-spinning analysis - if instead we ran the full analysis these agree nicely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "id": "SB4AqmTaygvU",
        "tags": []
      },
      "outputs": [],
      "source": [
        "parameters = dict(mass_1=36.2, mass_2=29.1)\n",
        "result_short.plot_corner(parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMvEC-yMwBr3"
      },
      "source": [
        "Earlier we discussed the \"correlation\" - in this plot we start to see the correlation between $m_1$ and $m_2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAkeCM3ywBr3"
      },
      "source": [
        "### Meta data\n",
        "The result object also stores meta data, like the priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "YsiX1-r9wBr3"
      },
      "outputs": [],
      "source": [
        "result_short.priors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3rftaARwBr3"
      },
      "source": [
        "and details of the analysis itself:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "2MoUgd3FwBr3"
      },
      "outputs": [],
      "source": [
        "result_short.sampler_kwargs[\"nlive\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo9_r5lSwBr4"
      },
      "source": [
        "Finally, we can also get out the Bayes factor for the signal vs. Gaussian noise. This is a good indicator of how much the hypothesis that the analyzed segment contains a binary black hole signal is reliable compared to that assuming it just contains noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "K5ufWz83wBr4"
      },
      "outputs": [],
      "source": [
        "print(\"ln Bayes factor = {} +/- {}\".format(\n",
        "    result_short.log_bayes_factor, result_short.log_evidence_err))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "B723ZKzrwBr4"
      },
      "source": [
        "## Challenge questions\n",
        "First, let's take a closer look at the result obtained with the run above. What are the means of the chirp mass and mass ratio distributions? What are the medians of the distributions for the components masses? You can use `np.mean` and `np.median` to calculate these.\n",
        "\n",
        "### Optional questions\n",
        "Now let's expand on this example a bit. Rerun the analysis above but change the prior on the distance from a delta function to `bilby.core.prior.PowerLaw(alpha=2., minimum=50., maximum=800., name='luminosity_distance')`. You should also replace `sample='unif'` with `sample=\"rwalk\", nact=1, walks=1` in your call to `bilby.run_sampler` above. This will take a bit longer than the original run, around ~20 minutes (something more if you are running on Colab or MyBinder). You also need to change the `label` in the call to `run_sampler` to avoid over-writing your results.\n",
        "\n",
        "What is the median reported value of the distance posterior? What is the new log Bayes factor for signal vs. Gaussian noise? Don't be alarmed if your results do not match the official LVC results, as these are not rigorous settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "J6jZLoP8wBr4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tuto_3.2_Parameter_estimation_for_compact_object_mergers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}